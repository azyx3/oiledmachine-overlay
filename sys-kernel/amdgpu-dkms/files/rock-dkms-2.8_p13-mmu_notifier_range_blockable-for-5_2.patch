--- a/amd/amdgpu/amdgpu_mn.c.orig	2019-10-03 05:15:41.494119783 -0700
+++ b/amd/amdgpu/amdgpu_mn.c	2019-10-03 15:10:00.800434860 -0700
@@ -280,14 +280,23 @@ static int amdgpu_mn_invalidate_range_st
 	/* TODO we should be able to split locking for interval tree and
 	 * amdgpu_mn_invalidate_node
 	 */
+
+#if DRM_VERSION_CODE >= DRM_VERSION(5, 2, 0) && DRM_VERSION_CODE < DRM_VERSION(5, 3, 0)
+	if (amdgpu_mn_read_lock(amn, mmu_notifier_range_blockable(range)))
+#elif DRM_VERSION_CODE < DRM_VERSION(5, 2, 0)
 	if (amdgpu_mn_read_lock(amn, range->blockable))
+#endif
 		return -EAGAIN;
 
 	it = interval_tree_iter_first(&amn->objects, range->start, end);
 	while (it) {
 		struct amdgpu_mn_node *node;
 
+#if DRM_VERSION_CODE >= DRM_VERSION(5, 2, 0) && DRM_VERSION_CODE < DRM_VERSION(5, 3, 0)
+		if (!mmu_notifier_range_blockable(range)) {
+#elif DRM_VERSION_CODE < DRM_VERSION(5, 2, 0)
 		if (!range->blockable) {
+#endif
 			amdgpu_mn_read_unlock(amn);
 			return -EAGAIN;
 		}
@@ -323,7 +332,11 @@ static int amdgpu_mn_invalidate_range_st
 	/* notification is exclusive, but interval is inclusive */
 	end = range->end - 1;
 
+#if DRM_VERSION_CODE >= DRM_VERSION(5, 2, 0) && DRM_VERSION_CODE < DRM_VERSION(5, 3, 0)
+	if (amdgpu_mn_read_lock(amn, mmu_notifier_range_blockable(range)))
+#elif DRM_VERSION_CODE < DRM_VERSION(5, 2, 0)
 	if (amdgpu_mn_read_lock(amn, range->blockable))
+#endif
 		return -EAGAIN;
 
 	it = interval_tree_iter_first(&amn->objects, range->start, end);
@@ -331,7 +344,11 @@ static int amdgpu_mn_invalidate_range_st
 		struct amdgpu_mn_node *node;
 		struct amdgpu_bo *bo;
 
+#if DRM_VERSION_CODE >= DRM_VERSION(5, 2, 0) && DRM_VERSION_CODE < DRM_VERSION(5, 3, 0)
+		if (!mmu_notifier_range_blockable(range)) {
+#elif DRM_VERSION_CODE < DRM_VERSION(5, 2, 0)
 		if (!range->blockable) {
+#endif
 			amdgpu_mn_read_unlock(amn);
 			return -EAGAIN;
 		}
