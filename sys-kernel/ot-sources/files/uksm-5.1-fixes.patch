--- a/mm/memory.c.orig	2019-05-11 12:42:52.850441965 -0700
+++ b/mm/memory.c	2019-05-11 12:56:51.490294160 -0700
@@ -2545,11 +2545,17 @@ static vm_fault_t do_wp_page(struct vm_f
 	 * Take out anonymous pages first, anonymous shared vmas are
 	 * not dirty accountable.
 	 */
+#ifndef CONFIG_UKSM
 	if (PageAnon(vmf->page)) {
+#else
+	if (PageAnon(vmf->page) && !PageKsm(vmf->page)) {
+#endif
 		int total_map_swapcount;
+#ifndef CONFIG_UKSM
 		if (PageKsm(vmf->page) && (PageSwapCache(vmf->page) ||
 					   page_count(vmf->page) != 1))
 			goto copy;
+#endif
 		if (!trylock_page(vmf->page)) {
 			get_page(vmf->page);
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
@@ -2564,6 +2570,7 @@ static vm_fault_t do_wp_page(struct vm_f
 			}
 			put_page(vmf->page);
 		}
+#ifndef CONFIG_UKSM
 		if (PageKsm(vmf->page)) {
 			bool reused = reuse_ksm_page(vmf->page, vmf->vma,
 						     vmf->address);
@@ -2573,6 +2580,7 @@ static vm_fault_t do_wp_page(struct vm_f
 			wp_page_reuse(vmf);
 			return VM_FAULT_WRITE;
 		}
+#endif
 		if (reuse_swap_page(vmf->page, &total_map_swapcount)) {
 			if (total_map_swapcount == 1) {
 				/*
--- a/mm/ksm.c.orig	2019-05-11 11:23:31.559661827 -0700
+++ b/mm/ksm.c	2019-05-11 12:57:47.812814610 -0700
@@ -2657,6 +2657,7 @@ again:
 		goto again;
 }
 
+#ifndef CONFIG_UKSM
 bool reuse_ksm_page(struct page *page,
 		    struct vm_area_struct *vma,
 		    unsigned long address)
@@ -2706,6 +2707,7 @@ void ksm_migrate_page(struct page *newpa
 	}
 }
 #endif /* CONFIG_MIGRATION */
+#endif /* !CONFIG_UKSM */
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 static void wait_while_offlining(void)
--- a/include/linux/ksm.h.orig	2019-05-11 11:23:31.549661446 -0700
+++ b/include/linux/ksm.h	2019-05-11 12:54:20.024174233 -0700
@@ -49,8 +49,10 @@ struct page *ksm_might_need_to_copy(stru
 
 void rmap_walk_ksm(struct page *page, struct rmap_walk_control *rwc);
 void ksm_migrate_page(struct page *newpage, struct page *oldpage);
+#ifndef CONFIG_UKSM
 bool reuse_ksm_page(struct page *page,
 			struct vm_area_struct *vma, unsigned long address);
+#endif
 
 #ifdef CONFIG_KSM_LEGACY
 int __ksm_enter(struct mm_struct *mm);
@@ -111,11 +113,13 @@ static inline void rmap_walk_ksm(struct
 static inline void ksm_migrate_page(struct page *newpage, struct page *oldpage)
 {
 }
+#ifndef CONFIG_UKSM
 static inline bool reuse_ksm_page(struct page *page,
 			struct vm_area_struct *vma, unsigned long address)
 {
 	return false;
 }
+#endif /* !CONFIG_UKSM */
 #endif /* CONFIG_MMU */
 #endif /* !CONFIG_KSM */
 
