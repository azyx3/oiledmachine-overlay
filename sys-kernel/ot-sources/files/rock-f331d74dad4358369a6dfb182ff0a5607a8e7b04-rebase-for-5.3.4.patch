--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c.orig	2019-10-23 19:31:18.157612226 -0700
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c	2019-10-23 19:35:50.213099319 -0700
@@ -1252,6 +1252,7 @@ const struct drm_ioctl_desc amdgpu_ioctl
 	DRM_IOCTL_DEF_DRV(AMDGPU_GEM_VA, amdgpu_gem_va_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(AMDGPU_GEM_OP, amdgpu_gem_op_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(AMDGPU_GEM_USERPTR, amdgpu_gem_userptr_ioctl, DRM_AUTH|DRM_RENDER_ALLOW)
+	DRM_IOCTL_DEF_DRV(AMDGPU_GEM_DGMA, amdgpu_gem_dgma_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
 };
 const int amdgpu_max_kms_ioctl = ARRAY_SIZE(amdgpu_ioctls_kms);
 
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c.orig	2019-10-23 19:31:18.163612458 -0700
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c	2019-10-23 19:35:50.214099358 -0700
@@ -1608,40 +1608,52 @@ static int amdgpu_vm_bo_split_mapping(st
 		dma_addr_t *dma_addr = NULL;
 		uint64_t max_entries;
 		uint64_t addr, last;
+		uint64_t count;
 
 		if (nodes) {
 			addr = nodes->start << PAGE_SHIFT;
 			max_entries = (nodes->size - pfn) *
-				AMDGPU_GPU_PAGES_IN_CPU_PAGE;
-		} else {
-			addr = 0;
-			max_entries = S64_MAX;
-		}
-
-		if (pages_addr) {
-			uint64_t count;
-
+				(PAGE_SIZE / AMDGPU_GPU_PAGE_SIZE);
+		switch (mem->mem_type) {
+			case TTM_PL_TT:
+				max_entries = min(max_entries, 16ull * 1024ull);
 			for (count = 1;
-			     count < max_entries / AMDGPU_GPU_PAGES_IN_CPU_PAGE;
-			     ++count) {
+				 count < max_entries / AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+				 ++count){
 				uint64_t idx = pfn + count;
-
-				if (pages_addr[idx] !=
-				    (pages_addr[idx - 1] + PAGE_SIZE))
+					if (pages_addr[idx] !=
+						(pages_addr[idx - 1] + PAGE_SIZE))
 					break;
-			}
-
-			if (count < min_linear_pages) {
-				addr = pfn << PAGE_SHIFT;
+				}
+				if (count < min_linear_pages) {
+					addr = pfn << PAGE_SHIFT;
+					dma_addr = pages_addr;
+				} else {
+					addr = pages_addr[pfn];
+					max_entries = count;
+				}
+				break;
+			case AMDGPU_PL_DGMA_IMPORT:
+				addr = 0;
+				max_entries = min(max_entries, 16ull * 1024ull);
 				dma_addr = pages_addr;
-			} else {
-				addr = pages_addr[pfn];
-				max_entries = count * AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+				break;
+			case AMDGPU_PL_DGMA:
+				addr += vram_base_offset +
+					adev->mman.bdev.man[mem->mem_type].gpu_offset -
+					adev->mman.bdev.man[TTM_PL_VRAM].gpu_offset;
+				addr += pfn << PAGE_SHIFT;
+				break;
+			case TTM_PL_VRAM:
+				addr += vram_base_offset;
+				addr += pfn << PAGE_SHIFT;
+				break;
+			default:
+				break;
 			}
-
-		} else if (flags & AMDGPU_PTE_VALID) {
-			addr += vram_base_offset;
-			addr += pfn << PAGE_SHIFT;
+		} else {
+			addr = 0;
+			max_entries = S64_MAX;
 		}
 
 		last = min((uint64_t)mapping->last, start + max_entries - 1);
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c.orig	2019-10-23 19:31:18.154612110 -0700
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c	2019-10-23 19:36:38.194947396 -0700
@@ -987,7 +987,6 @@ static int amdgpu_device_check_arguments
 		 * the minimum allowed but aligned size of 32GB */
 		amdgpu_vm_size_aligned = 32;
 		while (amdgpu_vm_size > amdgpu_vm_size_aligned)
-	amdgpu_direct_gma_size = min(amdgpu_direct_gma_size, 96);
 			amdgpu_vm_size_aligned *= 2;
 
 		amdgpu_vm_size = amdgpu_vm_size_aligned;
@@ -1013,6 +1012,7 @@ static int amdgpu_device_check_arguments
 	}
 
 	adev->firmware.load_type = amdgpu_ucode_get_load_type(adev, amdgpu_fw_load_type);
+	amdgpu_direct_gma_size = min(amdgpu_direct_gma_size, 96);
 
 	return ret;
 }
